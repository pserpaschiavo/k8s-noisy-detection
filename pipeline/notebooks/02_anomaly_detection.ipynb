{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e969f0ce",
   "metadata": {},
   "source": [
    "# Detecção Avançada de Anomalias em Experimentos de Noisy Neighbors\n",
    "\n",
    "Este notebook demonstra o uso das funcionalidades de detecção de anomalias para análise de experimentos de noisy neighbors no Kubernetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90365904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ajustar o path para importar os módulos do pipeline\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "# Importar módulos necessários\n",
    "from pipeline.data_processing.consolidation import load_experiment_data, select_tenants\n",
    "from pipeline.data_processing.time_normalization import normalize_time\n",
    "from pipeline.data_processing.aggregation import aggregate_by_time\n",
    "from pipeline.analysis.anomaly_detection import detect_anomalies_isolation_forest, \\\n",
    "                                               detect_anomalies_local_outlier_factor, \\\n",
    "                                               detect_change_points, \\\n",
    "                                               detect_pattern_changes, \\\n",
    "                                               detect_anomalies_ensemble\n",
    "from pipeline.visualization.plots import plot_metric_with_anomalies, \\\n",
    "                                        plot_multivariate_anomalies, \\\n",
    "                                        plot_change_points, \\\n",
    "                                        plot_metric_by_phase, plot_phase_comparison # Added for EDA\n",
    "from pipeline.config import METRIC_DISPLAY_NAMES, TENANT_COLORS, VISUALIZATION_CONFIG # Added for EDA\n",
    "\n",
    "# Configurar visualização\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c85a9",
   "metadata": {},
   "source": [
    "## 1. Carregar e Preparar os Dados\n",
    "\n",
    "Primeiro vamos carregar os dados de um experimento de exemplo e preparar os dados para análise de anomalias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3439ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para os dados do experimento\n",
    "experiment_path = '../../demo-data/demo-experiment-3-rounds/'\n",
    "\n",
    "# Carregar dados do experimento\n",
    "metrics_data, experiment_info = load_experiment_data(experiment_path)\n",
    "\n",
    "print(f\"Experimento carregado: {experiment_info['name']}\")\n",
    "print(f\"Métricas disponíveis: {list(metrics_data.keys())}\")\n",
    "print(f\"Tenants disponíveis: {experiment_info['tenants']}\")\n",
    "print(f\"Fases do experimento: {experiment_info['phases']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2712e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar métricas para análise\n",
    "selected_metrics = ['cpu_usage', 'memory_usage']\n",
    "selected_data = {k: metrics_data[k] for k in selected_metrics if k in metrics_data}\n",
    "\n",
    "# Selecionar tenants específicos (opcional)\n",
    "tenants_of_interest = ['tenant-a', 'tenant-b']\n",
    "selected_data = {k: select_tenants(df, tenants_of_interest) for k, df in selected_data.items()}\n",
    "\n",
    "# Normalizar o tempo para facilitar a análise\n",
    "selected_data = {k: normalize_time(df, experiment_info) for k, df in selected_data.items()}\n",
    "\n",
    "# Adicionar coluna de tempo decorrido em minutos para facilitar visualizações\n",
    "for k, df in selected_data.items():\n",
    "    df['elapsed_minutes'] = (df['datetime'] - df['datetime'].min()).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd90305",
   "metadata": {},
   "source": [
    "## 1.A. Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section performs a basic exploratory data analysis on the loaded and prepared data.\n",
    "We will focus on a specific metric to understand its behavior across different tenants and experiment phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a metric for the EDA section\n",
    "# You can change this to any of the selected_metrics\n",
    "if selected_metrics:\n",
    "    metric_name_for_eda = selected_metrics[0]\n",
    "else:\n",
    "    metric_name_for_eda = 'cpu_usage' # Fallback if selected_metrics is empty\n",
    "\n",
    "print(f\"--- EDA for metric: {metric_name_for_eda} ---\")\n",
    "\n",
    "if metric_name_for_eda in selected_data:\n",
    "    df_eda = selected_data[metric_name_for_eda]\n",
    "    print(\"\\nData Shape (for EDA metric):\")\n",
    "    print(df_eda.shape)\n",
    "    print(\"\\nData Types (for EDA metric):\")\n",
    "    print(df_eda.dtypes)\n",
    "    print(\"\\nDescriptive Statistics (for 'value' column of EDA metric):\")\n",
    "    if 'value' in df_eda.columns and not df_eda.empty:\n",
    "        print(df_eda['value'].describe())\n",
    "    elif df_eda.empty:\n",
    "        print(\"DataFrame for EDA metric is empty, cannot show descriptive statistics.\")\n",
    "    else:\n",
    "        print(\"'value' column not found in EDA metric DataFrame.\")\n",
    "    print(\"\\nFirst 5 rows (for EDA metric):\")\n",
    "    print(df_eda.head())\n",
    "else:\n",
    "    print(f\"Metric {metric_name_for_eda} not found in selected_data. Skipping EDA for this metric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e5568",
   "metadata": {},
   "source": [
    "### Visualize Metric Over Time by Phase (EDA)\n",
    "\n",
    "Using `plot_metric_by_phase` to visualize how the selected metric changes over time for each tenant, with phase distinctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metric_name_for_eda in selected_data and not selected_data[metric_name_for_eda].empty:\n",
    "    df_eda = selected_data[metric_name_for_eda]\n",
    "    fig_metric_by_phase_eda = plot_metric_by_phase(\n",
    "        df=df_eda,\n",
    "        metric_name=metric_name_for_eda,\n",
    "        time_column='elapsed_minutes', # or 'datetime'\n",
    "        show_phase_markers=True\n",
    "    )\n",
    "    plt.suptitle(f\"EDA: {METRIC_DISPLAY_NAMES.get(metric_name_for_eda, metric_name_for_eda)} Over Time by Phase\", fontsize=16)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"DataFrame for {metric_name_for_eda} is empty or not found. Skipping EDA plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c68f8",
   "metadata": {},
   "source": [
    "**Insights from EDA plot:**\n",
    "- Observe the baseline behavior of the metric for each tenant.\n",
    "- Identify how the metric changes during the 'Attack' phase (if applicable based on data).\n",
    "- Assess the recovery during the 'Recovery' phase (if applicable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4ebbb",
   "metadata": {},
   "source": [
    "### Phase Comparison Plot (EDA)\n",
    "\n",
    "To compare phases, we first need to aggregate the data by tenant and phase to get summary statistics (e.g., mean, std) for the metric in each phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metric_name_for_eda in selected_data and not selected_data[metric_name_for_eda].empty and 'value' in selected_data[metric_name_for_eda].columns:\n",
    "    df_eda = selected_data[metric_name_for_eda]\n",
    "    # Aggregate data: calculate mean and std of 'value' for each tenant and phase\n",
    "    aggregated_data_eda = df_eda.groupby(['tenant', 'phase'])['value'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    print(f\"\\nAggregated Data for Phase Comparison (EDA metric: {metric_name_for_eda}):\")\n",
    "    print(aggregated_data_eda.head())\n",
    "\n",
    "    fig_phase_comparison_eda = plot_phase_comparison(\n",
    "        df=aggregated_data_eda,\n",
    "        metric_name=metric_name_for_eda,\n",
    "        value_column='mean',\n",
    "        error_column='std' # Optional: if you want error bars\n",
    "    )\n",
    "    plt.suptitle(f\"EDA: Phase Comparison for {METRIC_DISPLAY_NAMES.get(metric_name_for_eda, metric_name_for_eda)}\", fontsize=16)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"DataFrame for {metric_name_for_eda} is empty or 'value' column missing. Skipping EDA phase comparison plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f892d7",
   "metadata": {},
   "source": [
    "**Insights from Phase Comparison Plot (EDA):**\n",
    "- Directly compare the average (or other aggregate) metric value across phases for each tenant.\n",
    "- Quantify the impact of different phases relative to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ee186",
   "metadata": {},
   "source": [
    "This concludes the initial exploratory data analysis. The subsequent sections will delve into more advanced anomaly detection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592ad14",
   "metadata": {},
   "source": [
    "## 2. Detecção de Anomalias usando Isolation Forest\n",
    "\n",
    "O Isolation Forest é um algoritmo eficiente para detectar anomalias baseado na premissa de que anomalias são mais fáceis de isolar do que observações normais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f70114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar métrica para análise\n",
    "metric_name = 'cpu_usage'\n",
    "df = selected_data[metric_name].copy()\n",
    "\n",
    "# Aplicar Isolation Forest para detectar anomalias\n",
    "df_with_anomalies = detect_anomalies_isolation_forest(\n",
    "    df, \n",
    "    metric_column='value', \n",
    "    contamination=0.05,  # Proporção esperada de anomalias\n",
    "    group_by=['tenant', 'phase']  # Agrupar por tenant e fase\n",
    ")\n",
    "\n",
    "# Verificar quantas anomalias foram detectadas\n",
    "anomaly_count = df_with_anomalies['is_anomaly_if'].sum()\n",
    "total_points = len(df_with_anomalies)\n",
    "print(f\"Detectadas {anomaly_count} anomalias em {total_points} pontos ({anomaly_count/total_points*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os resultados para um tenant específico\n",
    "tenant = 'tenant-a'\n",
    "tenant_data = df_with_anomalies[df_with_anomalies['tenant'] == tenant]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.scatter(tenant_data['elapsed_minutes'], tenant_data['value'], \n",
    "            c=tenant_data['is_anomaly_if'].map({True: 'red', False: 'blue'}),\n",
    "            alpha=0.6, s=30)\n",
    "plt.title(f'Detecção de Anomalias com Isolation Forest - {metric_name.upper()} para {tenant}')\n",
    "plt.xlabel('Tempo Decorrido (minutos)')\n",
    "plt.ylabel('Valor')\n",
    "plt.colorbar(plt.cm.ScalarMappable(cmap='coolwarm'), \n",
    "             label='Score de Anomalia')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe578e39",
   "metadata": {},
   "source": [
    "## 3. Detecção de Anomalias usando Local Outlier Factor (LOF)\n",
    "\n",
    "O LOF identifica anomalias baseando-se na densidade local dos pontos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar LOF para detectar anomalias\n",
    "df_with_lof = detect_anomalies_local_outlier_factor(\n",
    "    df, \n",
    "    metric_column='value', \n",
    "    n_neighbors=20,  # Número de vizinhos a considerar\n",
    "    contamination=0.05,  # Proporção esperada de anomalias\n",
    "    group_by=['tenant', 'phase']  # Agrupar por tenant e fase\n",
    ")\n",
    "\n",
    "# Verificar quantas anomalias foram detectadas\n",
    "anomaly_count = df_with_lof['is_anomaly_lof'].sum()\n",
    "total_points = len(df_with_lof)\n",
    "print(f\"Detectadas {anomaly_count} anomalias em {total_points} pontos ({anomaly_count/total_points*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os resultados para um tenant específico\n",
    "tenant = 'tenant-a'\n",
    "tenant_data = df_with_lof[df_with_lof['tenant'] == tenant]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.scatter(tenant_data['elapsed_minutes'], tenant_data['value'], \n",
    "            c=tenant_data['is_anomaly_lof'].map({True: 'red', False: 'blue'}),\n",
    "            alpha=0.6, s=30)\n",
    "plt.title(f'Detecção de Anomalias com LOF - {metric_name.upper()} para {tenant}')\n",
    "plt.xlabel('Tempo Decorrido (minutos)')\n",
    "plt.ylabel('Valor')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3bb2f7",
   "metadata": {},
   "source": [
    "## 4. Detecção de Pontos de Mudança\n",
    "\n",
    "Os pontos de mudança são momentos nos quais as propriedades estatísticas de uma série temporal mudam significativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar pontos de mudança\n",
    "df_with_changes, change_info = detect_change_points(\n",
    "    df, \n",
    "    metric_column='value',\n",
    "    time_column='elapsed_minutes',\n",
    "    method='pelt',  # Algoritmo PELT (Pruned Exact Linear Time)\n",
    "    model='l2',  # Modelo de custo L2 (erro quadrático)\n",
    "    min_size=10,  # Tamanho mínimo de segmento\n",
    "    penalty=3,  # Penalidade para controlar o número de pontos\n",
    "    group_by=['tenant']  # Agrupar por tenant\n",
    ")\n",
    "\n",
    "# Exibir informações sobre os pontos detectados\n",
    "for group, info in change_info.items():\n",
    "    if isinstance(group, tuple):\n",
    "        group_name = group[0]  # Para tuples (quando group_by é usado)\n",
    "    else:\n",
    "        group_name = group\n",
    "    print(f\"Grupo: {group_name}\")\n",
    "    print(f\"  Número de pontos de mudança: {info['n_change_points']}\")\n",
    "    if info['change_point_times']:\n",
    "        print(f\"  Tempos dos pontos de mudança: {[round(t, 2) for t in info['change_point_times']]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a845d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os pontos de mudança\n",
    "tenant = 'tenant-a'\n",
    "tenant_data = df_with_changes[df_with_changes['tenant'] == tenant]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(tenant_data['elapsed_minutes'], tenant_data['value'], 'b-', alpha=0.7)\n",
    "\n",
    "# Destacar pontos de mudança\n",
    "change_points = tenant_data[tenant_data['is_change_point']]\n",
    "plt.scatter(change_points['elapsed_minutes'], change_points['value'], \n",
    "            color='red', s=80, marker='o', label='Pontos de Mudança')\n",
    "\n",
    "plt.title(f'Detecção de Pontos de Mudança - {metric_name.upper()} para {tenant}')\n",
    "plt.xlabel('Tempo Decorrido (minutos)')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275116a",
   "metadata": {},
   "source": [
    "## 5. Detecção de Mudanças de Padrão\n",
    "\n",
    "Essa abordagem identifica mudanças nos padrões de comportamento usando clustering de séries temporais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12206cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar múltiplas métricas para análise de padrões\n",
    "metrics_to_analyze = ['cpu_usage', 'memory_usage']\n",
    "df_multi = pd.DataFrame()\n",
    "\n",
    "for metric in metrics_to_analyze:\n",
    "    if metric in selected_data:\n",
    "        temp_df = selected_data[metric].copy()\n",
    "        temp_df = temp_df.rename(columns={'value': metric})\n",
    "        \n",
    "        # Mesclar com o DataFrame principal\n",
    "        if df_multi.empty:\n",
    "            df_multi = temp_df[['tenant', 'phase', 'round', 'datetime', 'elapsed_minutes', metric]]\n",
    "        else:\n",
    "            df_multi = pd.merge(\n",
    "                df_multi, \n",
    "                temp_df[['tenant', 'datetime', metric]], \n",
    "                on=['tenant', 'datetime'], \n",
    "                how='outer'\n",
    "            )\n",
    "\n",
    "# Verificar o DataFrame resultante\n",
    "print(f\"Formato do DataFrame: {df_multi.shape}\")\n",
    "df_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar mudanças de padrão\n",
    "pattern_changes = detect_pattern_changes(\n",
    "    df_multi,\n",
    "    metrics=metrics_to_analyze,\n",
    "    time_column='elapsed_minutes',\n",
    "    window_size=15,  # Tamanho da janela para capturar padrões\n",
    "    n_clusters=3,    # Número de clusters de padrões\n",
    "    group_by=['tenant']  # Agrupar por tenant\n",
    ")\n",
    "\n",
    "# Exibir mudanças de padrão detectadas\n",
    "if not pattern_changes.empty:\n",
    "    print(f\"Detectadas {len(pattern_changes)} mudanças de padrão\")\n",
    "    display(pattern_changes)\n",
    "else:\n",
    "    print(\"Nenhuma mudança de padrão detectada ou dados insuficientes para análise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c990cb1",
   "metadata": {},
   "source": [
    "## 6. Abordagem de Ensemble para Detecção de Anomalias\n",
    "\n",
    "Combinar múltiplos algoritmos pode melhorar a precisão da detecção de anomalias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ecdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar ensemble de detecção de anomalias\n",
    "df_ensemble, change_info_ensemble = detect_anomalies_ensemble(\n",
    "    df, \n",
    "    metric_column='value',\n",
    "    time_column='elapsed_minutes',\n",
    "    contamination=0.05,\n",
    "    group_by=['tenant', 'phase']\n",
    ")\n",
    "\n",
    "# Verificar resultados\n",
    "anomaly_count = df_ensemble['is_anomaly'].sum()\n",
    "total_points = len(df_ensemble)\n",
    "print(f\"Detectadas {anomaly_count} anomalias em {total_points} pontos ({anomaly_count/total_points*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b58e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os resultados do ensemble para um tenant específico\n",
    "tenant = 'tenant-a'\n",
    "tenant_data = df_ensemble[df_ensemble['tenant'] == tenant]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plotar os valores\n",
    "plt.plot(tenant_data['elapsed_minutes'], tenant_data['value'], 'b-', alpha=0.5, label='Valores')\n",
    "\n",
    "# Destacar anomalias do método ensemble\n",
    "anomalies = tenant_data[tenant_data['is_anomaly']]\n",
    "plt.scatter(anomalies['elapsed_minutes'], anomalies['value'], \n",
    "            color='red', s=50, marker='o', label='Anomalias (Ensemble)')\n",
    "\n",
    "# Destacar pontos de mudança\n",
    "change_points = tenant_data[tenant_data['is_change_point']]\n",
    "plt.scatter(change_points['elapsed_minutes'], change_points['value'], \n",
    "            color='purple', s=80, marker='X', label='Pontos de Mudança')\n",
    "\n",
    "plt.title(f'Detecção com Ensemble - {metric_name.upper()} para {tenant}')\n",
    "plt.xlabel('Tempo Decorrido (minutos)')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d5857",
   "metadata": {},
   "source": [
    "## 7. Comparação entre Diferentes Tipos de Anomalias\n",
    "\n",
    "Analisar diferenças entre os tipos de anomalias detectadas por cada algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contabilizar os diferentes tipos de anomalias\n",
    "anomaly_types = pd.DataFrame({\n",
    "    'Isolation Forest': df_ensemble['is_anomaly_if'].sum(),\n",
    "    'LOF': df_ensemble['is_anomaly_lof'].sum(),\n",
    "    'Ensemble': df_ensemble['is_anomaly'].sum(),\n",
    "    'Pontos de Mudança': df_ensemble['is_change_point'].sum()\n",
    "}, index=['Total'])\n",
    "\n",
    "# Calcular estatísticas por tenant\n",
    "tenant_stats = []\n",
    "for tenant in df_ensemble['tenant'].unique():\n",
    "    tenant_data = df_ensemble[df_ensemble['tenant'] == tenant]\n",
    "    tenant_stats.append({\n",
    "        'Tenant': tenant,\n",
    "        'Isolation Forest': tenant_data['is_anomaly_if'].sum(),\n",
    "        'LOF': tenant_data['is_anomaly_lof'].sum(),\n",
    "        'Ensemble': tenant_data['is_anomaly'].sum(),\n",
    "        'Pontos de Mudança': tenant_data['is_change_point'].sum(),\n",
    "        'Total de Pontos': len(tenant_data)\n",
    "    })\n",
    "\n",
    "tenant_anomaly_stats = pd.DataFrame(tenant_stats).set_index('Tenant')\n",
    "\n",
    "# Exibir estatísticas\n",
    "print(\"Estatísticas Globais:\")\n",
    "display(anomaly_types)\n",
    "\n",
    "print(\"\\nEstatísticas por Tenant:\")\n",
    "display(tenant_anomaly_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a distribuição dos scores de anomalia\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Criar um DataFrame para a visualização\n",
    "scores_df = pd.DataFrame({\n",
    "    'Isolation Forest': df_ensemble['anomaly_score_if'],\n",
    "    'LOF': df_ensemble['anomaly_score_lof'],\n",
    "    'Ensemble': df_ensemble['anomaly_score_combined']\n",
    "})\n",
    "\n",
    "# Plotar histograma\n",
    "scores_df.hist(bins=50, alpha=0.7, figsize=(14, 5))\n",
    "plt.suptitle('Distribuição dos Scores de Anomalia', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Plotar boxplots para comparar distribuições\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=scores_df)\n",
    "plt.title('Comparação dos Scores de Anomalia entre Algoritmos')\n",
    "plt.ylabel('Score de Anomalia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77280c",
   "metadata": {},
   "source": [
    "## 8. Relação entre Anomalias e Fases do Experimento\n",
    "\n",
    "Analisar como a distribuição de anomalias varia entre as diferentes fases do experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63146f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise por fase\n",
    "phase_stats = []\n",
    "for phase in df_ensemble['phase'].unique():\n",
    "    phase_data = df_ensemble[df_ensemble['phase'] == phase]\n",
    "    phase_stats.append({\n",
    "        'Fase': phase,\n",
    "        'Total de Pontos': len(phase_data),\n",
    "        'Anomalias (IF)': phase_data['is_anomaly_if'].sum(),\n",
    "        'Anomalias (LOF)': phase_data['is_anomaly_lof'].sum(),\n",
    "        'Anomalias (Ensemble)': phase_data['is_anomaly'].sum(),\n",
    "        'Pontos de Mudança': phase_data['is_change_point'].sum(),\n",
    "        '% Anomalias': phase_data['is_anomaly'].sum() / len(phase_data) * 100\n",
    "    })\n",
    "\n",
    "phase_anomaly_stats = pd.DataFrame(phase_stats).set_index('Fase')\n",
    "display(phase_anomaly_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2651c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a distribuição de anomalias por fase\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Preparar dados para visualização\n",
    "phases = phase_anomaly_stats.index\n",
    "if_anomalies = phase_anomaly_stats['Anomalias (IF)']\n",
    "lof_anomalies = phase_anomaly_stats['Anomalias (LOF)']\n",
    "ensemble_anomalies = phase_anomaly_stats['Anomalias (Ensemble)']\n",
    "\n",
    "# Plotar barras agrupadas\n",
    "x = np.arange(len(phases))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, if_anomalies, width, label='Isolation Forest')\n",
    "plt.bar(x, lof_anomalies, width, label='LOF')\n",
    "plt.bar(x + width, ensemble_anomalies, width, label='Ensemble')\n",
    "\n",
    "plt.xlabel('Fase')\n",
    "plt.ylabel('Número de Anomalias')\n",
    "plt.title('Distribuição de Anomalias por Fase')\n",
    "plt.xticks(x, phases)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Porcentagem de anomalias por fase\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(phases, phase_anomaly_stats['% Anomalias'], color='darkred')\n",
    "plt.xlabel('Fase')\n",
    "plt.ylabel('% de Pontos Anômalos')\n",
    "plt.title('Porcentagem de Anomalias por Fase')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02288275",
   "metadata": {},
   "source": [
    "## 9. Análise de Correlação entre Anomalias e Métricas\n",
    "\n",
    "Investigar se há correlação entre diferentes métricas nas anomalias detectadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc701f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mesclar resultados de anomalias de diferentes métricas\n",
    "anomaly_results = {}\n",
    "\n",
    "for metric_name in selected_metrics:\n",
    "    if metric_name in selected_data:\n",
    "        df = selected_data[metric_name].copy()\n",
    "        \n",
    "        # Aplicar detecção de anomalias\n",
    "        df_anomalies, _ = detect_anomalies_ensemble(\n",
    "            df, \n",
    "            metric_column='value',\n",
    "            time_column='elapsed_minutes',\n",
    "            contamination=0.05,\n",
    "            group_by=['tenant']\n",
    "        )\n",
    "        \n",
    "        # Armazenar resultados\n",
    "        anomaly_results[metric_name] = df_anomalies\n",
    "\n",
    "# Criar um DataFrame consolidado com anomalias para diferentes métricas\n",
    "consolidated_anomalies = None\n",
    "\n",
    "for metric_name, df in anomaly_results.items():\n",
    "    # Selecionar apenas colunas relevantes\n",
    "    temp_df = df[['tenant', 'phase', 'datetime', 'elapsed_minutes', 'value', 'is_anomaly']]\n",
    "    temp_df = temp_df.rename(columns={\n",
    "        'value': f'{metric_name}_value', \n",
    "        'is_anomaly': f'{metric_name}_anomaly'\n",
    "    })\n",
    "    \n",
    "    # Mesclar com o DataFrame consolidado\n",
    "    if consolidated_anomalies is None:\n",
    "        consolidated_anomalies = temp_df\n",
    "    else:\n",
    "        consolidated_anomalies = pd.merge(\n",
    "            consolidated_anomalies,\n",
    "            temp_df,\n",
    "            on=['tenant', 'phase', 'datetime', 'elapsed_minutes'],\n",
    "            how='outer'\n",
    "        )\n",
    "\n",
    "# Verificar o DataFrame resultante\n",
    "if consolidated_anomalies is not None:\n",
    "    print(f\"Formato do DataFrame consolidado: {consolidated_anomalies.shape}\")\n",
    "    display(consolidated_anomalies.head())\n",
    "else:\n",
    "    print(\"Nenhum resultado de anomalia disponível para análise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar a correlação entre anomalias em diferentes métricas\n",
    "if consolidated_anomalies is not None and len(selected_metrics) >= 2:\n",
    "    # Criar tabela de contingência para anomalias\n",
    "    anomaly_columns = [f'{metric}_anomaly' for metric in selected_metrics]\n",
    "    contingency_table = pd.crosstab(\n",
    "        consolidated_anomalies[anomaly_columns[0]], \n",
    "        consolidated_anomalies[anomaly_columns[1]],\n",
    "        rownames=['Anomalia em ' + selected_metrics[0]],\n",
    "        colnames=['Anomalia em ' + selected_metrics[1]]\n",
    "    )\n",
    "    \n",
    "    # Exibir tabela de contingência\n",
    "    print(f\"Tabela de Contingência para Anomalias em {selected_metrics[0]} vs {selected_metrics[1]}:\")\n",
    "    display(contingency_table)\n",
    "    \n",
    "    # Calcular coeficiente phi (correlação para variáveis binárias)\n",
    "    from scipy.stats import contingency\n",
    "    \n",
    "    try:\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "        n = contingency_table.sum().sum()\n",
    "        phi = np.sqrt(chi2 / n)\n",
    "        \n",
    "        print(f\"\\nCoeficiente Phi (correlação entre anomalias): {phi:.4f}\")\n",
    "        print(f\"Valor-p: {p:.4f} ({'Significativo' if p < 0.05 else 'Não significativo'} ao nível de 5%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao calcular estatísticas: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b2cc2",
   "metadata": {},
   "source": [
    "## 10. Conclusões e Próximos Passos\n",
    "\n",
    "Este notebook demonstrou diferentes técnicas de detecção de anomalias que podem ser aplicadas a experimentos de noisy neighbors:\n",
    "\n",
    "1. **Isolation Forest**: Eficiente para detectar pontos isolados que se desviam do comportamento normal\n",
    "2. **Local Outlier Factor**: Sensível a anomalias locais baseadas em densidade\n",
    "3. **Detecção de Pontos de Mudança**: Identifica mudanças significativas no comportamento da série temporal\n",
    "4. **Detecção de Mudanças de Padrão**: Analisa mudanças em padrões multivariados usando clustering\n",
    "5. **Ensemble de Algoritmos**: Combina diferentes abordagens para uma detecção mais robusta\n",
    "\n",
    "### Próximos Passos:\n",
    "\n",
    "- Validar as anomalias detectadas com especialistas de domínio\n",
    "- Correlacionar as anomalias com eventos específicos no cluster Kubernetes\n",
    "- Implementar métricas de avaliação para medir a precisão da detecção\n",
    "- Expandir a análise para incluir mais métricas e relações entre tenants\n",
    "- Integrar técnicas de explicabilidade para entender melhor as causas das anomalias"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
